{
    "contents" : "---\ntitle: \"A trip to Asymptopia\"\nauthor: \"Brian Caffo, Jeff Leek, Roger Peng\"\nhighlighter: highlight.js\noutput: pdf_document\njob: Johns Hopkins Bloomberg School of Public Health\nlogo: bloomberg_shield.png\nmode: selfcontained\nhitheme: tomorrow\nsubtitle: Statistical Inference\nframework: io2012\nurl:\n  assets: ../../assets\n  lib: ../../librariesNew\nwidgets: mathjax\n---\n## Asymptotics\n* Asymptotics is the term for the behavior of statistics as the sample size (or some other relevant quantity) limits to infinity (or some other relevant number)\n* (Asymptopia is my name for the land of asymptotics, where everything works out well and there's no messes. The land of infinite data is nice that way.)\n* Asymptotics are incredibly useful for simple statistical inference and approximations \n* (Not covered in this class) Asymptotics often lead to nice understanding of procedures\n* Asymptotics generally give no assurances about finite sample performance\n* Asymptotics form the basis for frequency interpretation of probabilities \n  (the long run proportion of times an event occurs)\n\n\n---\n\n## Limits of random variables\n\n- Fortunately, for the sample mean there's a set of powerful results\n- These results allow us to talk about the large sample distribution\nof sample means of a collection of $iid$ observations\n- The first of these results we inuitively know\n  - It says that the average limits to what its estimating, the population mean\n  - It's called the Law of Large Numbers\n  - Example $\\bar X_n$ could be the average of the result of $n$ coin flips (i.e. the sample proportion of heads)\n    - As we flip a fair coin over and over, it evetually converges to the\n    true probability of a head\n    The LLN forms the basis of frequency style thinking\n\n\n---\n## Law of large numbers in action\n```{r, fig.height=5, fig.width=5}\nn <- 10000; means <- cumsum(rnorm(n)) / (1  : n); library(ggplot2)\ng <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) \ng <- g + geom_hline(yintercept = 0) + geom_line(size = 2) \ng <- g + labs(x = \"Number of obs\", y = \"Cumulative mean\")\ng\n```\n\n\n---\n## Law of large numbers in action, coin flip\n```{r, fig.height=5, fig.width=5}\nmeans <- cumsum(sample(0 : 1, n , replace = TRUE)) / (1  : n)\ng <- ggplot(data.frame(x = 1 : n, y = means), aes(x = x, y = y)) \ng <- g + geom_hline(yintercept = 0.5) + geom_line(size = 2) \ng <- g + labs(x = \"Number of obs\", y = \"Cumulative mean\")\ng\n```\n\n\n\n---\n## Discussion\n- An estimator is **consistent** if it converges to what you want to estimate\n  - The LLN says that the sample mean of iid sample is\n  consistent for the population mean\n  - Typically, good estimators are consistent; it's not too much to ask that if we go to the trouble of collecting an infinite amount of data that we get the right answer\n- The sample variance and the sample standard deviation\nof iid random variables are consistent as well\n\n---\n\n## The Central Limit Theorem\n\n- The **Central Limit Theorem** (CLT) is one of the most important theorems in statistics\n- For our purposes, the CLT states that the distribution of averages of iid variables (properly normalized) becomes that of a standard normal as the sample size increases\n- The CLT applies in an endless variety of settings\n- The result is that \n$$\\frac{\\bar X_n - \\mu}{\\sigma / \\sqrt{n}}=\n\\frac{\\sqrt n (\\bar X_n - \\mu)}{\\sigma}\n= \\frac{\\mbox{Estimate} - \\mbox{Mean of estimate}}{\\mbox{Std. Err. of estimate}}$$ has a distribution like that of a standard normal for large $n$.\n- (Replacing the standard error by its estimated value doesn't change the CLT)\n- The useful way to think about the CLT is that \n$\\bar X_n$ is approximately\n$N(\\mu, \\sigma^2 / n)$\n\n\n\n---\n\n## Example\n\n- Simulate a standard normal random variable by rolling $n$ (six sided)\n- Let $X_i$ be the outcome for die $i$\n- Then note that $\\mu = E[X_i] = 3.5$\n- $Var(X_i) = 2.92$ \n- SE $\\sqrt{2.92 / n} = 1.71 / \\sqrt{n}$\n- Lets roll $n$ dice, take their mean, subtract off 3.5,\nand divide by $1.71 / \\sqrt{n}$ and repeat this over and over\n\n\n---\n## Result of our die rolling experiment\n\n```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}\nnosim <- 1000\ncfunc <- function(x, n) sqrt(n) * (mean(x) - 3.5) / 1.71\ndat <- data.frame(\n  x = c(apply(matrix(sample(1 : 6, nosim * 10, replace = TRUE), \n                     nosim), 1, cfunc, 10),\n        apply(matrix(sample(1 : 6, nosim * 20, replace = TRUE), \n                     nosim), 1, cfunc, 20),\n        apply(matrix(sample(1 : 6, nosim * 30, replace = TRUE), \n                     nosim), 1, cfunc, 30)\n        ),\n  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))\ng <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(alpha = .20, binwidth=.3, colour = \"black\", aes(y = ..density..)) \ng <- g + stat_function(fun = dnorm, size = 2)\ng + facet_grid(. ~ size)\n```\n\n\n---\n## Coin CLT\n\n - Let $X_i$ be the $0$ or $1$ result of the $i^{th}$ flip of a possibly unfair coin\n- The sample proportion, say $\\hat p$, is the average of the coin flips\n- $E[X_i] = p$ and $Var(X_i) = p(1-p)$\n- Standard error of the mean is $\\sqrt{p(1-p)/n}$\n- Then\n$$\n    \\frac{\\hat p - p}{\\sqrt{p(1-p)/n}}\n$$\nwill be approximately normally distributed\n- Let's flip a coin $n$ times, take the sample proportion\nof heads, subtract off .5 and multiply the result by\n$2 \\sqrt{n}$ (divide by $1/(2 \\sqrt{n})$)\n\n---\n## Simulation results\n```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}\nnosim <- 1000\ncfunc <- function(x, n) 2 * sqrt(n) * (mean(x) - 0.5) \ndat <- data.frame(\n  x = c(apply(matrix(sample(0:1, nosim * 10, replace = TRUE), \n                     nosim), 1, cfunc, 10),\n        apply(matrix(sample(0:1, nosim * 20, replace = TRUE), \n                     nosim), 1, cfunc, 20),\n        apply(matrix(sample(0:1, nosim * 30, replace = TRUE), \n                     nosim), 1, cfunc, 30)\n        ),\n  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))\ng <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = \"black\", aes(y = ..density..)) \ng <- g + stat_function(fun = dnorm, size = 2)\ng + facet_grid(. ~ size)\n```\n\n---\n## Simulation results, $p = 0.9$\n```{r, echo = FALSE, fig.width=9, fig.height = 6, fig.align='center'}\nnosim <- 1000\ncfunc <- function(x, n) sqrt(n) * (mean(x) - 0.9) / sqrt(.1 * .9)\ndat <- data.frame(\n  x = c(apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 10, replace = TRUE), \n                     nosim), 1, cfunc, 10),\n        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 20, replace = TRUE), \n                     nosim), 1, cfunc, 20),\n        apply(matrix(sample(0:1, prob = c(.1,.9), nosim * 30, replace = TRUE), \n                     nosim), 1, cfunc, 30)\n        ),\n  size = factor(rep(c(10, 20, 30), rep(nosim, 3))))\ng <- ggplot(dat, aes(x = x, fill = size)) + geom_histogram(binwidth=.3, colour = \"black\", aes(y = ..density..)) \ng <- g + stat_function(fun = dnorm, size = 2)\ng + facet_grid(. ~ size)\n```\n\n---\n## Galton's quincunx \n\nhttp://en.wikipedia.org/wiki/Bean_machine#mediaviewer/File:Quincunx_(Galton_Box)_-_Galton_1889_diagram.png\n\n<img src=\"fig/quincunx.png\" height=\"450\"></img>\n\n---\n\n## Confidence intervals\n\n- According to the CLT, the sample mean, $\\bar X$, \nis approximately normal with mean $\\mu$ and sd $\\sigma / \\sqrt{n}$\n- $\\mu + 2 \\sigma /\\sqrt{n}$ is pretty far out in the tail\n(only 2.5% of a normal being larger than 2 sds in the tail)\n- Similarly, $\\mu - 2 \\sigma /\\sqrt{n}$ is pretty far in the left tail (only 2.5% chance of a normal being smaller than 2 sds in the tail)\n- So the probability $\\bar X$ is bigger than $\\mu + 2 \\sigma / \\sqrt{n}$\nor smaller than $\\mu - 2 \\sigma / \\sqrt{n}$ is 5%\n    - Or equivalently, the probability of being between these limits is 95%\n- The quantity $\\bar X \\pm 2 \\sigma /\\sqrt{n}$ is called\na 95% interval for $\\mu$\n- The 95% refers to the fact that if one were to repeatly\nget samples of size $n$, about 95% of the intervals obtained\nwould contain $\\mu$\n- The 97.5th quantile is 1.96 (so I rounded to 2 above)\n- 90% interval you want (100 - 90) / 2 = 5% in each tail \n  - So you want the 95th percentile (1.645)\n\n\n---\n## Give a confidence interval for the average height of sons\nin Galton's data\n```{r}\nlibrary(UsingR);data(father.son); x <- father.son$sheight\n(mean(x) + c(-1, 1) * qnorm(.975) * sd(x) / sqrt(length(x))) / 12\n```\n\n---\n\n## Sample proportions\n\n- In the event that each $X_i$ is $0$ or $1$ with common success probability $p$ then $\\sigma^2 = p(1 - p)$\n- The interval takes the form\n$$\n    \\hat p \\pm z_{1 - \\alpha/2}  \\sqrt{\\frac{p(1 - p)}{n}}\n$$\n- Replacing $p$ by $\\hat p$ in the standard error results in what is called a Wald confidence interval for $p$\n- For 95% intervals\n$$\\hat p \\pm \\frac{1}{\\sqrt{n}}$$ \nis a quick CI estimate for $p$\n\n---\n## Example\n* Your campaign advisor told you that in a random sample of 100 likely voters,\n  56 intent to vote for you. \n  * Can you relax? Do you have this race in the bag?\n  * Without access to a computer or calculator, how precise is this estimate?\n* `1/sqrt(100)=0.1` so a back of the envelope calculation gives an approximate 95% interval of `(0.46, 0.66)`\n  * Not enough for you to relax, better go do more campaigning!\n* Rough guidelines, 100 for 1 decimal place, 10,000 for 2, 1,000,000 for 3.\n```{r}\nround(1 / sqrt(10 ^ (1 : 6)), 3)\n```\n\n\n\n---\n## Binomial interval\n\n```{r}\n.56 + c(-1, 1) * qnorm(.975) * sqrt(.56 * .44 / 100)\nbinom.test(56, 100)$conf.int\n```\n\n---\n\n## Simulation\n\n```{r}\nn <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000\ncoverage <- sapply(pvals, function(p){\n  phats <- rbinom(nosim, prob = p, size = n) / n\n  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  mean(ll < p & ul > p)\n})\n\n```\n\n\n---\n## Plot of the results (not so good)\n```{r, echo=FALSE, fig.align='center', fig.height=6, fig.width=6}\nggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(.75, 1.0)\n````\n\n---\n## What's happening?\n- $n$ isn't large enough for the CLT to be applicable\nfor many of the values of $p$\n- Quick fix, form the interval with \n$$\n\\frac{X + 2}{n + 4}\n$$\n- (Add two successes and failures, Agresti/Coull interval)\n\n---\n## Simulation\nFirst let's show that coverage gets better with $n$\n\n```{r}\nn <- 100; pvals <- seq(.1, .9, by = .05); nosim <- 1000\ncoverage2 <- sapply(pvals, function(p){\n  phats <- rbinom(nosim, prob = p, size = n) / n\n  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  mean(ll < p & ul > p)\n})\n\n```\n\n---\n## Plot of coverage for $n=100$\n```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}\nggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage2)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)\n```\n\n---\n## Simulation\nNow let's look at $n=20$ but adding 2 successes and failures\n```{r}\nn <- 20; pvals <- seq(.1, .9, by = .05); nosim <- 1000\ncoverage <- sapply(pvals, function(p){\n  phats <- (rbinom(nosim, prob = p, size = n) + 2) / (n + 4)\n  ll <- phats - qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  ul <- phats + qnorm(.975) * sqrt(phats * (1 - phats) / n)\n  mean(ll < p & ul > p)\n})\n```\n\n\n---\n## Adding 2 successes and 2 failures\n(It's a little conservative)\n```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}\nggplot(data.frame(pvals, coverage), aes(x = pvals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ ylim(.75, 1.0)\n````\n\n---\n\n## Poisson interval\n* A nuclear pump failed 5 times out of 94.32 days, give a 95% confidence interval for the failure rate per day?\n* $X \\sim Poisson(\\lambda t)$.\n* Estimate $\\hat \\lambda = X/t$\n* $Var(\\hat \\lambda) = \\lambda / t$ \n* $\\hat \\lambda / t$ is our variance estimate\n\n---\n## R code\n```{r}\nx <- 5; t <- 94.32; lambda <- x / t\nround(lambda + c(-1, 1) * qnorm(.975) * sqrt(lambda / t), 3)\npoisson.test(x, T = 94.32)$conf\n```\n\n\n---\n## Simulating the Poisson coverage rate\nLet's see how this interval performs for lambda\nvalues near what we're estimating\n```{r}\nlambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000\nt <- 100\ncoverage <- sapply(lambdavals, function(lambda){\n  lhats <- rpois(nosim, lambda = lambda * t) / t\n  ll <- lhats - qnorm(.975) * sqrt(lhats / t)\n  ul <- lhats + qnorm(.975) * sqrt(lhats / t)\n  mean(ll < lambda & ul > lambda)\n})\n```\n\n\n\n---\n## Covarage\n(Gets really bad for small values of lambda)\n```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}\nggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95)+ylim(0, 1.0)\n````\n\n\n\n---\n## What if we increase t to 1000?\n```{r, fig.align='center', fig.height=6, fig.width=6, echo=FALSE}\nlambdavals <- seq(0.005, 0.10, by = .01); nosim <- 1000\nt <- 1000\ncoverage <- sapply(lambdavals, function(lambda){\n  lhats <- rpois(nosim, lambda = lambda * t) / t\n  ll <- lhats - qnorm(.975) * sqrt(lhats / t)\n  ul <- lhats + qnorm(.975) * sqrt(lhats / t)\n  mean(ll < lambda & ul > lambda)\n})\nggplot(data.frame(lambdavals, coverage), aes(x = lambdavals, y = coverage)) + geom_line(size = 2) + geom_hline(yintercept = 0.95) + ylim(0, 1.0)\n```\n\n\n---\n## Summary\n- The LLN states that averages of iid samples \nconverge to the population means that they are estimating\n- The CLT states that averages are approximately normal, with\ndistributions\n  - centered at the population mean \n  - with standard deviation equal to the standard error of the mean\n  - CLT gives no guarantee that $n$ is large enough\n- Taking the mean and adding and subtracting the relevant\nnormal quantile times the SE yields a confidence interval for the mean\n  - Adding and subtracting 2 SEs works for 95% intervals\n- Confidence intervals get wider as the coverage increases\n(why?)\n- Confidence intervals get narrower with less variability or\nlarger sample sizes\n- The Poisson and binomial case have exact intervals that\ndon't require the CLT\n  - But a quick fix for small sample size binomial calculations is to add 2 successes and failures\n",
    "created" : 1410433298557.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2007302259",
    "id" : "7EE6902E",
    "lastKnownWriteTime" : 1410447121,
    "path" : "~/GitHub/courses/06_StatisticalInference/07_Asymptopia/index.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_markdown"
}